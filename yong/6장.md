# 6장 카프카 내부 메커니즘

주요 키워드

- 카프카 컨트롤러
- 카프카에서 복제(replication)가 작동하는 방식
- 카프카가 프로듀서와 컨슈머의 요청을 처리하는 방식
- 카프카가 저장을 처리하는 방식(파일 형식, 인덱스 등)

위 주제들을 학습함으로써 튜닝하는 설정을 명확한 의도를 가지고 설정 값을 잡아주는데 도움이 된다

## 6.1 클러스터 멤버십

카프카는 현재 클러스터의 멤버인 브로커들의 목록을 유지하기 위해 아파치 주키퍼를 사용하며 브로커의 설정 파일에 정의되었거나 자동으로 고유한 식별자를 가진다

브로커 프로세스는 시작될 때마다 주키퍼에 Ephermeral 노드의 형태로 ID를 등록하고 컨트롤러를 포함한 카프카 브로커들과 몇몇 생태계 툴들은 브로커가 등록되는 주키퍼의 /brokers/ids 경로를 구독함으로써 브로커가 추가되거나 제거 될때마다 알림을 받는다.

새 브로커는 자신의 ID를 등록하려하지만 이미 동일한 브로커 ID를 갖는 ZNode가 있기 때문에 실패한다

브로커와 주키퍼 간의 연결이 끊어질 경우 주키퍼에서 삭제되고 카프카 컴포넌트들은 브로커가 내려갔음을 알게 된다

브로커가 정지하면 ZNode도 삭제 되지만 브로커의 ID는 다른 자료구조에 남아있게 된다

만약 특정한 브로커가 완전히 유실되어 동일한 ID를 가진 새로운 브로커가 투입 되면 유실되었던 브로커를 대신하여 이전 브로커의 토픽과 파티션을 할당 받음

## 6.2 컨트롤러

컨트롤러는 일반적인 카프카 브로커의 기능에 더해서 파티션 리더를 선출하는 역할을 추가적으로 맡음

클러스터에서 시작 → 브로커가 주키퍼의 /controller에 Ephemeral 노드를 생성 시 → 컨트롤러 

노드가 이미 존재하면 컨트롤러 노드가 존재한다는것이며 이를 확인하기 위해 노트에 와치를 설정함 

→ 클러스터 안에 한 번에 단 한 개의 컨트롤러만 있도록 보장

콘트롤러가 멈추거나 주키퍼와의 연결이 끊어지거나, zookeeper.session.timerout.ms에 설정 된 값보다 오랫동안 주키퍼에 하트비트를 전송하지 않아도 Ephemeral 노드 삭제

주키퍼는 새로운 컨트롤러가 선출 될 때마다 주키퍼의 조건적 증가 연산에 의해 증가된 에포크(epoch) 값을 전달받게 된다 

브로커는 현재 컨트롤러의 에포크 값을 알고 있기에 만약 더 낮은 에포크 값을 가진 컨트롤러로부터 메시지를 받을 경우 무시 한다  ⇒ 낙관적 락과 유사한듯

좀비 컨트롤러 : 새로운 컨트롤러가 선출 되었지만 이전 컨트롤러가 작업을 재개할 경우

브로커가 컨트롤러가 되면, 적재 작업 지연 될 수 있기에 비동기 API를 사용해서 수행하고 읽기 요청을 여러 단계로 나눠서 주키퍼로 보내지만 파티션 수가 많은 클러스터에서는 적재 작업이 몇초 씩 걸릴 수도 있다.

브로커가 나가면 : 콘트롤러는 맡던 모든 파티션을 새로운 브로커 할당 → 주키퍼에 쓰기 → 파티션의 리플리카와 모든 브로커의 LeaderAndISR 요청을 보냄 
⇒ 블록체인의 서명과 비슷한게 아닌지..?

새 리더는 쓰기, 읽기 요청을 처리하고 팔로워들은 리더로부터 메시지를 복제함
→ 클러스터 안의 모든 브로커는 클러스터 내 전체 브로커와 레플리카 맵을 포함하는 MetadataCache를 가짐

리더로 선출될 자격을 얻기 위해서는 그 전에 리더에 쓰여진 메시지를 따라 잡아야 함

요약 → 콘트롤러는 브로커가 클러스터에 추가/제거 될때마다 파티션과 레플리카 중에서 리더 선출을 책임
컨토롤러는 서로 다른 2개의 브로커가 자신이 현재 컨트롤러라 생각하는 ‘스필릿 브레인’을 방지하기 위해 에포크 번호를 사용

## 6.2.1 KRaft : 카프카의 새로운 래프트 기반 컨트롤러

2019년부터 준비한 프로젝트로 2.8부터 실험했고 3.3부터는 정식

개발 이유

1. 콘트롤러가 주키퍼에 메타 데이터를 쓰는 작업은 동기적으로 이루어지지만, 브로커 메시지를 보내는 작업은 비동기적으로 이루어진다. 또한, 주키퍼로부터 업데이트를 받는 과정 역시 비동기적으로 이루어진다.
그렇기 때문에 브로커, 컨트롤러, 주키퍼 간에 메타데이터 불일치가 발생할 수 있으며, 잡아내기도 어렵다
2. 컨트롤러가 재시작될 때마다 주키퍼로부터 모든 브로커와 파티션에 대한 메타데이터를 읽어와야 한다.
그러고 나서 이 메타데이터를 모든 브로커로 전송한다. 몇 년 동안 노력했지만, 이 부분은 여전히 병목이다. 즉, 파티션과 브로커의 수가 증가함에 따라 컨트롤러 재시작은 더욱 더 느려진다
3. 메타 데이터 소유권 관련된 내부 아키텍처는 그리 좋지 못하다. 즉, 어떤 작업은 컨트롤러가 하고, 다른 건 브로커가 하고, 나머지는 주키퍼가 직접 한다.
4. 주키퍼는 그 자체로 분산 시스템이며, 카프카와 마찬가지로 운영을 위해서는 어느 넝도 기반 지식이 있어야 한다. 그렇기 때문에 카프카를 사용하려는 개발자들은 두 개의 분산 시스템에 대해 배워야 한다 

현재 아키텍처의 주키퍼의 두 가지 기능

1. 컨트롤러 선출
2. 클러스터 메타 데이터 저장(운영중인 브로커, 설정, 토픽, 파티션, 레플리카 관련 정보)

### 새로운 컨트롤러의 핵심 아이디어

→ 카프카 자체에 사용자가 상태를 이벤트 스트림으로 나타낼 수 있도록 로그 기반 아키텍처를 도입

⇒ 일라스틱 서치랑 비슷한게 아닐지?

래프트 알고리즘을 사용 : 컨트롤러 노드들은 외부 시스템에 의존하지 않고 자체적으로 리더를 선출할 수 있게 됨

→ 메타 데이터 로그의 리더 역할을 맡고 있는 컨트롤러는 액티브 컨트롤러라고 부른다

컨트롤러의 변경 사항 적용 방식 : 브로커들이 새로 도입 된 API MetadataFetch API로 pull 함

→ 브로커는 추후 시동 시간을 줄이기 위해 메타데이터를 디스크에 저장한다. 파티션이 아무리 많아도

브로커 프로세스는 시작 시 주키퍼가 아닌 컨트롤러 쿼럼(quorum)에 등록하고 운영자가 해제하지 않는한 유지함

→ 브로커가 종료되어도 오프라인 상태일 뿐 유지 됨

→ 온라인 상태여도 메타데이터가 최신 상태가 아니면 펜스 상태로 클라이언트 요청을 처리 할 수 없다

⇒ 좀비 컨트롤러랑 뭐가 다르지..? 

→ 최신 상태에서 너무 떨어지는 바람에 자신이 리더가 아니라는 것조차 인식 못하는 브로커에 쓰는것을 방지

@ 컨트롤러? 어느 컨토를러

KRaft 도입 전 카프카

- 주키퍼 프로세스 : 카프카 클러스터의 동적 메타데이터를 저장하는 역할.
홀수 개의 프로세스가 하나의 쿼럼을 구성하며, 사용자 입장에서 명시적으로 보이지는 않지만 이 중에서 저장된 데이터의 업데이트 작업을 담당하는 리더 프로세스가 하나 있다
- 카프카 프로세스 : 카프카 데이터를 저장하는 역할.
이들 중에서 리더 파티션을 결정하는 역할을 하는 프로세스를 컨트롤러라 한다

KRaft 이후에는 ‘주키퍼 프로세스’가 제거 됨 : 카프카 프로세스만 존재 

다만, 카프카 프로세스는 아래 역할 중 하나 혹은 둘 다 겸할 수 있음

- 컨트롤러 : 카프카 콘트롤러의 동적 메타데이터를 저장하는 역할
1개 이상의 프로세스가 하나의 쿼름을 구성하며, 이들 중에서 저장된 데이터의 업데이트 및 조회 작업을 담당하는 프로세스를 액티브 컨트롤러라고 한다
- 브로커 : 카프카 데이터를 저장하는 역할
하나의 컨트롤러 쿼럼을 사용하는 브로커들이 모여 하나의 클러스터를 이룬다

→ KRaft 이전에는 ‘브로커’ = ‘카프카 프로세스’ 동의어 
→ KRaft 이후에는 ‘브로커’ = 프로세스가 맡는 특정한 ‘역할’에 가까움

→ ‘컨트롤러’ = 동적 메타데이터를 저장 하는 역할

@ 어떻게 KRaft로 옮길 수 있을까

Pre-KRaft와 Post-KRaft를 이용하면 할 수 있지만 다운타임이 생기므로 기존 데이터를 미러링해서 가져가는 방법을 고려해야함

@ KRaft 모드 사용법

### 설정 변경

3.3.0부터 KRaft 모드 예제 설정 파일 제공 되며 config/kraft 디렉토리에 있음

- broker.properties : 브로커 역할을 할 때
- controller.properties : 해당 서버가 컨트롤러 역할을 하도록 설정
- server.properties : 둘 다 할 경우에 대한 파일

- process.roles : controller or broker and controller, broker
- node.id
- controller.quorum.voters : 사용할 컨트롤러 쿼럼 지정 
→ {컨트롤러의 node.id값}@{컨트롤러의 호스트명}:{포트명}
→ 9092 브로커 기본값
→ 9093 컨트롤러 기본값
- listeners : 브로커 리스너 역할과 컨트롤러 리스너 설정
→ listeners=PLAINTEXT://:9092
→ listeners=CONTROLLER://:9093
→ listeners=PLAINTEXT://:9092,CONTROLLER://:9093
- log.dir

## 6.3 복제

복제는 카프카 아키텍처의 핵심

실제로 카프카는 ‘분산되고, 분할되고, 복제된 커밋 로그 서비스’로 표현 됨

북제가 중요한 이유 : 개별 노드의 장애가 발생하는 상황에서 카프카의 신뢰성과 지속성을 보장하는 방식

카프카에 저장되는 데이터 = 토픽 단위로 조직화 

각 토픽은 1개 이상의 파티션으로 분할되며, 각 파티션은 다시 다수의 레플리카를 가질 수 있음

각각의 레플리카는 브로커에 저장됨

하나의 브로커는(서로 다른 토픽과 파티션에 속하는) 수백 개에서 심지어 수천 개의 레플리카를 저장함

### 리더 레플리카

- 각 파티션에 하나의 레플리카
- 일관성 보장
- 모든 쓰기 요청은 리더 레플리카에 주어짐
- 클라이언트들은 리더 레플리카나 팔로워로부터 레코드를 읽어올 수 있음

### 팔로워 레플리카

- 파티션에 속한 모든 레플리카 중 리더를 제외한 모든 레플리카를 칭함
- 팔로워는 클라이언트 요청을 처리할 수 없음
- 리더 레플리카의 메시지를 복제하여 최신 상태 유지
- 리더 레플리카 크래쉬면 리더 선출로 승격

리더 레플리카와 동기화를 위해 읽기 요청을 보내고 이 요청은 컨슈머가 메시지 읽어오기 위해 시용하는 그요청이다???

인-싱크 레플리카 : 지속적으로 최신 메시지를 요청하고 있는 레플리카

replica.lag.time.max.ms : 팔로워 레플리카, 읽기 요청을 보내지 않거나 뒤처진 상태로 있는 일정 시간

auto.leader.rebalance.enable=true : 리더 였던 리플리카는 ‘선호’ 표시를 함 

## 6.4 요청 처리

카프카 브로커가 하는 일 : 클라이언트, 파티션 레플리카, 컨트롤러가 파티션 리더에게 보내는 요청을 처리

모든 요청은 표준 헤더를 가짐

- 요청 유형 : API 키
- 요청 버전 : 브로커는 서로 다른 버전의 클라이언트로부터 요청을 받아 각각의 버전에 맞는 응답을 할 수 있음
- Correlation ID : 각각의 요청에 붙은 고유한 식별자. 응답이나 에러 로그에도 포함(트러블 슈팅에 사용)
- 클라이언트 ID : 요청을 보낸 애플리케이션을 식별하기 위해 사용

브로커는 연결을 받는 각 포트별로 억셉터(acceptor) 스레드 하나씩 실행

억셉터 : 연결을 생성하고 들어온 요청을 프로세스 스레드(=네트워크 스레드)에 넘겨 처럼

프로세스 스레드 : 클라이언트 연결 → 요청 수신 → 요청 큐 → 응답 큐 → 클라이언트 송신

→ 컨슈머의 경우 브로커 쪽에 데이터가 준비되었을 때에만 응답을 보낼 수 있고 어드민 클라이언트의 경우 토픽 삭제가 진행중인 상황에서만 DeleteTopicreqeust 요청에 대한 응답을 보낼 수 있음
응답이 완료 될 떄까지 퍼거토리(purgatory)에 저장 

요청 큐에 들어오면 : I/O 쓰레드가 요청을 가져와서 처리하는 일을 담당함

- 쓰기 요청
- 읽기 요청
- 어드민 요청
- 

### 6.4.1 쓰기 요청

acks 설정 매개변수는 쓰기 작업이 성공한 것으로 간주되기 전 메시지에 대한 응받을 보내야 하는 브로커의 수를 가리키며 어느 시점에서 메시지가 ‘성공적으로 쓰여졌다’라고 간주되는지는 프로듀서 설정을 통해 바꿀 수 있음

- acks=1 : 리더만이 메시지를 받았을 때
- acks=all : 모든 인-싱크 레플리카들이 메시지를 받았을 때
- acks=0 : 메시지가 보내졌을 때 즉, 브로커의 응답을 기다리지 않음

파티션의 리더 레플리카를 가지고 있는 브로커가 해당 파티션에 대한 쓰기 요청을 받게 되면 몇가지 유효성 검증부터 한다

- 데이터를 보내고 있는 사용자가 토픽에 대한 쓰기 권한을 가지고 있는가?
- 요청에 지정되어 있는 acks 설정값이 올바른가 (0, 1, all)
- 만약 acks 설정 값이 all로 잡혀 있을 경우, 메시지를 안전하게 쓸 수 있을 만큼 충분한 인-싱크 레플리카가 있는가(현재 인-싱크 레플리카 수가 설정 된 값 아래로 내려가면 새로운 메시지를 받지 않도록 브로커를 설정해 줄 수 있다.)

검증 후 브로커는 새 메시지들을 로컬 디스크에 씀

언제 쓰여지는지 보장 없으며 카프카는 데이터가 디스크에 저장될 때까지 기다리지 않음

→ 메시지의 지속성을 위해 복제에 의존함

메시지가 파티션 리더에 쓰여지고 나면, 브로커는 acks 설정에 따라 응답을 내려보냄.

만약 0,1이면 바로 응답을 보내고 

all이면 일단 요청을 퍼거토리 버퍼에 저장 하고 팔로워 레플리카들이 메시지를 복제한 것을 확인 후 클라이언트에 응답함

### 6.4.2 읽기 요청

브로커는 쓰기 요청과 비슷하게 읽기 요청을 처리함

클라이언트는 브로커에 토픽, 파티션, 오프셋 목록 해당 메시지들을 달라는 요청을 함 

1. 요청은 요청에 지정된 파티션들의 리더를 맡고 있는 브로커에 전송
2. 클라이언트는 읽기 요청을 정확히 라우팅 할 수 있는 메타데이터 요청
3. 요청 받은 파티션 디러는 먼저 요청이 유효한지 확인
    1. 만약 클라이언트가 너무 오래되어 파티션에서 삭제 된 메시지나 아직 존재하지 않는 오프셋의 메시지를 요청할 경우 브로커는 에러를 응답으로 보내게 된다

1. 만약 오프셋이 존재한다면, 브로커는 파티션으로부터 클라이언트가 요청에 지정한 크기 한도만큼 메시지를 읽어서 클라이언트에게 보냄
2. 카프카는 클라이언트에게 보내는 메시지에 제로카피 최적화를 적용
    1. 파일에서 읽어 온 메시지들을 중간 버퍼를 거치지 않고 바로 네트워크 채널로 보내는 것
    2. 이 점이 클라이언트에게 데이터를 보내기 전에 로컬 캐시에 저장하는 데이터베이스와의 차이점
    3. 이 방식으로 데이터 복사하고 메모리 상 버퍼 관리를 위한 오버헤드가 사라짐

설정 중 데이터 양의 하한과 시간을 지정할 수 있음 

1. 10K 바이트가 쌓이면 결과 리턴 해라
2. 밀리초마다 데이터 요청을 리턴 해라

충분한 수의 레플리카에 복제가 완료되지 않은 메시지는 ‘불완전한’ 한 것으로 간주되고 리더 크래시가 나면 다른 레플리카가 리더 역할을 이어받는다면, 이 메시지들은 더 이상 카프카에 존재하지 않게 된다.

만약 클라이언트가 이렇게 리더에만 존재하는 메시지들을 읽을 수 있도록 한다면, 크래시 상황에서 일관성이 결여될 수 있다

replica.lag.time.max.ms 설정 값에 따라 제한 됨

→ 이 값은 인-싱크 상태로 판정되는 레플리카가 새 메시지를 복제하는 과정에서 지연될 수 있는 최대 시간임

→ 해당 시간 이상으로 지연 되면 아웃-오브-싱크-레플리카가 됨)

오베헤드 최소화를 위한 ‘읽기 세션 캐시 사용’ : 일곡자 하는 파티션의 집합이나 여기에 연관 된 메타 데이터는 여간해서는 잘 바뀌지 않는데데다가 많은 경우 리턴 해야 할 메타 데이터가 그렇게 많지도 않음

### 6.4.3 기타 요청

일반적인 클라이언트의 요청

- 메타데이터
- 쓰기
- 읽기

카프카에는 61개의 서로 다른 요청 유형을 정의하고 계속 늘어나고 있음

컨슈머 : 그룹 생성, 읽기 조정, 컨슈머 그룹 관리 등에만 15개가 있음

카프카 토픽에 해당 오프셋들을 저장하도록 변경 : OffsetCommit, OffsetFetch, ListOffsets

## 6.5 물리적 저장소

카프카의 기본 저장 단위는 파티션 레플리카
→ 파티션은 서로 다른 브로커들 사이에 분리될 수 없으며
→ 같은 브로커의 서로 다른 디스크에 분할 저장되는 것도 불가능

### 6.5.1 계층화 된 저장소

대량의 데이터를 저장하기 위한 목적으로 계층화 된 저장소를 추가함

계층 저장소를 만드는 이유

- 파티션별로 저장 가능한 데이터에는 한도가 있다. 결과적으로, 최대 보존 기한과 파티션 수는 제품의 요구 조건이 아닌 물리적인 디스크 크기에도 제한을 받는다
- 디스크와 클러스터 크기는 저장소 요구 조건에 의해 결정된다. 지연과 처리량이 주 고려사항일 경우 클러스터는 필요한 것 이상으로 커지는 경우가 많다. 이는 곧 비용으로 직결된다
- 예를 들어서 클러스터의 크기를 키우거나 줄일 때, 파티션의 위치를 다른 브로커로 옮기는 데 걸리는 시간은 파티션의 크기에 따라 결정된다. 파티션의 크기가 클수록 클러스터의 탄력성은 줄어든다. 클라우드 환경의 유연한 옵션을 활용할 수 있도록 최대한 탄력성을 가지는 것이 아키텍처 설계의 추세다.

계층 저장소 구분

- 로컬 : 현재의 카프카 저장소 계층과 똑같은 세그먼트를 저장하기 위해 카프카 브로커의 로컬 디스크 사용
- 원격 : 완료 된 로그 세그먼트를 저장하기 위해 HDFS나 S3와 같은 전용 저장소 시스템 사용

로컬 저장소는 원격 저장소에 비해 지연이 훨씬 짧다.

지연에 민감한 경우 로컬계층을 최신 데이터를 가져오록 하고 빠진 처리 결괄르 메꾸는 작업이나 장애에서 복구되고 있는 애플리케이션들은 로컬 계층에 있는 것보다 더 오래된 데이터를 필요로 하는 만큼 원격 계층에 있는 데이터가 전달 된다.

계층화된 저장소 기능의 이중화 덕에 카프카 클러스터의 메모리와 CPU에 상관없이 저장소를 확장할 수 있다.

이로 인해 카프카는 장기간용 저장 솔루션으로서의 역할을 할 수 있게 됨

→ 흔히들 하고 있는 카프카 데이터를 외부 저장소로 복사하는 별도의 데이터 파이프라인을 구축할 필요가 없음

### 6.5.2 파티션 할당

사용자가 토픽을 생성하면, 카프카는 우선 이 파티션을 브로커 중 하나에 할당함

ex) 

1. 브로커가 6개 
2. 파티션이 10개 
3. 복제 팩터가 3인 토픽
4. 30개의 파티션 레플리카를 6개의 브로커에 할당
    - 레플리카들을 가능한 한 브로커 간에 고르게 분산시킨다(브로커별로 5개의 레플리카를 할당)
    - 각 파티션에 대해 각각의 레플리카는 서로 다른 브로커에 배치
        - 만약 파티션 0의 리더가 브로커 2에 있다면, 팔로워들은 브로커 3과 4에 배치될 수 있음
        - 하지만 브로커 2에 팔로워가 또 배치되거나(브로커 2에 리더와 팔로워가 하나씩 함께 배치)
        - 브로커 3에 팔로워 두 개가 함께 배치 될수는 없다
    - 만약 브로커에 랙 정보가 설정되어 있다면(카프카 버전 0.10.0 이상에서 가능) 가능한 한 각 파티션의 레플리카들을 서로 다른 랙에 할당
        - 이렇게 하면 하나의 랙 전체가 작동 불능에 빠지더라도 파티션 전체가 사용 불능에 빠지는 사태 방지
    - 임의의 브로커(4라고 했을때)부터 각 브로커에 라운드 로빈 방식으로 파티션을 할당함으로써 리더 결정

### 6.5.3 파일 관리

보존은 카프카에 있어서 중요한 개념이다

카프카는 영구히 데이터를 저장하지도, 데이터를 지우기 전에 모든 컨슈머들이 메시지를 읽어갈 수 있도록 기다리지도 않는다. 대신 카프카 운영자는 각각의 토픽에 대해 보존 기한(retention period)을 설정할 수 있다.

즉 ‘이만큼 오래 도니 메시지는 지운다’ 혹은 ‘이 용량이 넘어가면 지운다’ 와 같은 설정 

현재 쓰여지고 있는 세그먼트를 액티브 세그먼트라고 부른다

### 6.5.4 파일 형식

각 세그먼트는 하나의 데이터 파일 형태로 저장된다.

파일 안에는 카프카의 메시지와 오프셋이 저장된다.

- 디스크에 저장되는 데이터의 형식은 사용자가 프로듀서를 통해서 브로커로 보내는
- 나중에 브로커로부터 컨슈머로 보내지는 메시지의 형식과 동일

로컬과 원격 저장 방식을 동일하게 함 

카프카 메시지는 사용자 페이로드와 시스템 헤더, 두부분으로 나뉨

사용자 페이로드 : 키, 밸류, 헤도 모음

버전 0.11부터 카프카 프로듀서는 언제나 메시지를 배치 단위로 전송

→ 만약 하나의 메시지만 보내려고 할 때는 약간의 오버헤드를 발생시킴

→ 배치당 2개 이상의 메시지를 보낼 경우 공간읠 절약하는 만큼 네트워크 대역폭과 디스크 공간을 덜 사용함 

→ linger.ms = 10 설정을 잡아 주었을 때 더 성능이 좋아지는 이유 중 하나임

→ 프로듀서에서 압축 기능을 사용할 경우 더 큰 배치를 전송할수록 네트워크를 통해 전송되고 브로커의 디스크에 저장되는 데이턱 더 잘 압축 됨

메시지 배치 헤더에는 아래가 포함됨

- 메시지 형식의 현재 버전을 가리키는 매직 넘버
- 배치에 포함된 첫 번쨰 메시지의 오프셋과 마지막 오프셋과의 차이(이 값들은 나중에 배치가 압착되어 일부 메시지가 삭제되더라도 보존됨), 프로듀서가 배치를 생성해서 전송하는 시점에서는 첫 번째 메시지의 오프셋이 0이 된다. 이 배치를 처음으로 저장하는 브로커(즉, 파티션 리더)가 이 값을 실제 오프셋으로 교체
- 첫 번째 메시지의 타임스탬프와 배치에서 가장 큰 타임스탬프, 타임스탬프 유형이 생성 시각이 아닌 추가 시각으로 잡혀 있을 경우, 브로커가 타임스탬프를 잡아주게 된다
- 바이트 단위로 표시된 배치의 크기
- 해당 배치를 받은 리더의 에포크 값, 이 값은 새 리더가 선출된 뒤 메시지를 절삭할 때 사용된다.
- 배치가 오염되지 않았음을 확인하기 위한 체크섬
- 서로 다른 속성을 표시하기 위한 16비트 : 압축 유형, 타임스탬프 유형(타임스탬프를 클라이언트가 지정할 수도 있고 브로커가 지정할 수도 있음), 배치가 트랜잭션의 일부 혹은 컨트롤 배치인 지의 여부
- 프로듀서 ID, 프로듀서 에포크, 그리고 배치의 첫 번째 시퀀스 넘버 : 이들은 모두 ‘정확히 한번’ 보장을 위해 사용된다
- 배치에 포함된 메시지들의 집함

각가의 레코드는 아래가 포함됨

- 바이트 단위로 표시된 레코드의 크기
- 속성 : 현재로서 레코드단위 속성은 없기에 사용되지 않음
- 현재 레코드의 오프셋과 배치 내 첫 번째 레코드의 오프셋과의 차이
- 현재 레코드의 타임스탬프와 배치 내 첫 번째 레코드의 타임스탬프와의 차이(밀리초)
- 사용자 페이로드 : 키, 밸류 헤더

각 레코드는 거의 오버헤드가 없으며 대부분 시스템 정보는 배치 단위에서 저장되어 있는 점에 주목

DumpLogSegment 툴을 이용하면 해당 정보를 볼 수 있음 

## 6.5.5 인덱스

카프카는 컨슈머가 임의의 사용 가능한 오프셋에서부터 메시지를 읽어오기 시작할수 있도록 한다.

브로커가 주어진 오프셋의 메시지를 빠르게 찾을 수 있도록 하기 위해 카프카는 각 파티션에 대해 오프셋을 유지함

이 인덱스는 오프셋과 세그먼트 파일 및 그 안에서의 위치를 매핑한다.

→ 카프카는 타임스탬프오 메시지 으포셋을 매핑하는 또 다른 인덱스를 가지고 있음 

→ 이 인덱스는 타임스탬프를 기준으로 메시지를 찾을 때 사용되며 카프카 스트림즈는 타임스탬프 기준 메시지 검색을 광범위하게 사용하며 몇몇 장애 복구 상황에서도 유용하게 사용됨

### 6.5.6 압착

카프카는 설정 된 기간 동안만 메시지를 저장하며 보존 시간이 지나간 메시지들은 삭제함

카프카는 두 가지 보존 정책이 있음

- 삭제 보존 정책 : 지정 된 보존 기한보다 더 오래 된 이벤트들을 삭제
- 압착 보존 정책 : 토픽에서 각 키의 가장 최근 값만 저장, null인 메시지가 있을 경우 압착 실패

→ 보존 기한과 압착 설정을 동시에 적용 : delete, compact

→ 지정된 보존 기한보다 오래 된 메시지들은 키에 대한 가장 최근값인경에도 삭제

### 6.5.7 압착 작동 원리

- 클린 : 이전에 압착된 적이 있었던 메시지들이 저장된다. 이 영역은 하나의 키마다 하나의 값만을 포함한다.(이 값은 이전 압착 작업 시점에서의 최신값이기도 하다.)
- 더티 : 마지막 압착 작업 이후 쓰여진 메시지들이 저장된다.

각 스레드는 전체 파티션 크기 대비 더티 메시지의 비율이 가장 높은 파티션을 골라서 압착한 뒤 클린 상태로 만듬

- 파티션을 압착하기 위해서 클리너 스레드는 파티션의 더티 영역을 읽어서 인-메모리 맵을 생성

### 6.5.8 삭제된 이벤트

만약 키별로 항상 최근 메시지가 보존된다면, 특정 키를 갖는 모든 메시지를 삭제하고 싶을 때는 어떻게 할까?

→ 사용자가 서비스를 탈퇴해서 해당 사용자의 모든 흔적을 시스템에서 지워야 할 법적인 의무가 생기는 것에 해당

→ 가장 최근 메시지조차 남기지 않고 시스템에서 특정 키를 완전히 삭제하려면, 해당 키값과 null 밸류값을 갖는 메시지를 써주면 됨
→ 카프카는 사전에 설정된 기간동안 이 특별한 메시지(톰스톤)를 보존할 것임

### 6.5.9 토픽은 언제 압착되는가?

삭제 정책이 현재의 액티브 세그먼트를 절대로 삭제하지 않는 것과 마찬가지로, 압착 정책 역시 현재의 액티브 세그먼트를 절대로 압착하지 않느다.

액티브 세그먼트가 아닌 세그먼트에 저장되어 있는 메시지만이 압착의 대상이 된다.

카프카는 토픽 내용물의 50%이상이 더티 렠드인 경우에만 압착을 시작

운영자들은 두 설정 매개변수를 사용해서 압착이 시작되는 시점을 조절

- min.compaction.lag.ms : 메시지가 쓰여진 뒤 압착될 때까지 지나가야 하는 최소 시간을 지정
- max.compaction.lag.ms : 메시지가 쓰여진 뒤 압착이 가능해질 때까지 딜레이 될 수 있는 최대 시간 지정
이 설정은 특정 기한 안에 압착이 반드시 실행된다는 것을 보장해야 하는 상황에서 자주 사용됨
