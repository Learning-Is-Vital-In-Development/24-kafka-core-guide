# 1장 카프카 입문

카프카의 기본 단위는 메시지이고 효율성을 위해 배치 단위로 저장한다

스키마 : 아파치 에어브로 사용을 선호

토픽과 파티션
저장 되는 메시지는 토픽 단위로 분류 = 데이터베이스의 테이블나 파일 시스템의 폴더
토픽은 여러개의 파티션으로 나뉨 = 로그 시스템에서는 하나의 로그를 의미 = 하나의 row,  FIFO 순
단일 파티션 안에서만 순서 보장되며 여러개의 파티션으 순서 보장 하지 않음

스트림 : 프로듀서로부터 컨슈머로의 하나의 데이터 흐름을 나타냄

프로듀서는 발행자 혹은 작성자라고 하며 발행/구독에서 새로운 메시지를 생성함
컨슈머는 구독자 혹은 독자라고 하며 발행/구독에서 메시지를 구독 혹은 읽는다
메시지 오프셋을 가지고 어디까지 읽었는지를 기록함

브로커는 하나의 카프카 서버를 의미하며 프로듀서로와 컨슈머의 요청을 처리하는 송수신 역할을 한다
클러스터의 일부로 카프카 브로커가 있고 하나의 클러스터 안에는 여러개의 브로커가 존재할 수 있고 하나의 브로커가 컨트롤러의 역할을 하며 모니터링과 관리 기능을 담당한다. 그리고 브로커 중 파티션 리더와 파티션 팔로워를 할당하면서 고가용성을 보장한다

카프카의 핵심 기능 중에는 일정 기간 동안 메시지를 지속성 있게 보존하는 기능이 있고 로그 압착(compaction)기능도 있음

미러 메이커는 데이터를 다른 클러스터로 복제하는데 사용되는 툴이다
근본적으로는 큐로 연결 된 컨슈머와 프로듀서에 불과함

카프카 장점

- 다중 프로듀서
- 다중 컨슈머
- 디스크 기반 보존
- 확장성
- 고성능
- 플랫폼 기능

카프카 이용사례

1. 활동 추적
2. 메시지 교환
3. 지표 및 로그 수집
4. 커밋 로그
5. 스트림 처리

# 2장 카프카 설치

tickTime : 2,000mili second

## 2.1.3 주키퍼 설치

- 주키퍼 앙상블 : 홀수 서버(3개, 5개...) -> 3개의 노드로 이루어진 앙상블에서 1개의 노드가 정지해도 문제 없이 작동

## 2.3 카프카 브로커 설정하기

1. [broker.id](http://broker.id/)
2. listeners
3. zookeeper.connect
4. log.dirs
5. num.recovery.threads.per.data.dir
6. auto.create.topics.enable
7. auto.leader.rebalance.enable
8. delete.topic.enable

### 2.3.2 토픽별 기본값

1. num.partitions
2. default.replication.factor
3. [log.retention.ms](http://log.retention.ms/)
4. log.retention.bytes
5. log.segment.bytes
6. [log.roll.ms](http://log.roll.ms/)
7. min.insync.replicas
8. message.max.bytes

### 2.4 하드웨어 선택

1. 디스크 처리량
2. 디스크 용량
3. 메모리
4. 네트워크
5. CPU

### 2.6 카프카 클러스터 설정

1. 브로커 개수 : 디스크 용량, 브로커당 레플리카 용량, CPU 용량, 네트워크 용량
2. 브로커 설정
    - 설정 시 필수 모든 브로커들이 동일한 zookeeper.connect 설정 값을 가져야 함
    - 클러스터 안의 모든 브로커가 유일한 [broker.id](http://broker.id/) 설정 값을 가져야 함
3. 운영체제 튜닝
    - 가상 메모리 : vm.swappiness 1로 설정 -> 어떠한 경우에도 스와핑하지 말라
    -> {파티션 수} x ({파티션수 / 세그먼트 크기}) + {브로커에 생성된 네트워크 연결 수}
    - 디스크
        - ctime, mtime, atime
    - 네트워킹
        - net.core.wmem_default와 net.core.rmem_default : 합리적 설정 값 131072(128KB)
        - net.core.wmem_max와 net.core.rmem_max : 합리적 설정 값 2097152(2MB)
4. 가비지 수집기 옵션
    - GIGC(가비지 수집기)의 성능 설정 두가지
        - MaxGCPauseMillis : 선호되는 중단 시간 지정
        - InitiatingHeapOccupancyPercent : 사이클을 시작하기 전까지 전체 힙에서 사용 가능한 비율을 백분율로 지정하며
        기본 값은 45

# 3장 카프카 프로듀서

카프카를 큐로서 사용하든, 메시지 버스나 데이터 저장 플랫폼으로 사용하든 간에 카프카를 사용할 때는 카프카에 데이터를 쓸 때 사용하는 프로듀서나 읽어 올 때 사용하는 컨슈머, 혹은 두 가지 기능 모두를 수행하는 애플리케이션을 생성해야 한다

## 3.1 프로듀서 개요

카프카 메시지 쓰는 순서

1. ProducerRecord 객체 생성
2. 저장 될 토픽과 밸류 지정 필수
3. 키와 파디션 지정 선택
4. 키와 값 객체가 네트워크 상에서 전송될 수 있도록 직렬화해서 바이트 배열로 변환하는 과정 수행, 만약 파티션이 명시되지 않았으면 해당 데이터를 파티셔너에게로 보냄, 파티셔너는 파티션을 결정하는 역할을 함, 파티션이 결정되어 메시지가 전송 될 토픽과 파티션이 확정되면 프로듀서는 이 레코드를 같은 토픽 파트션으로 전송 될 레코드들을, 모은 레코드 배치에 추가 함, 이후 별도의 스레드가 이 레코드 배치를 적절한 카프카 브로커에게 전송함

### 3.2 카프카 프로듀서 생성하기

필수 속성

1. bootstrap.servers : 브로커 목록
2. ket.serializer : 카프카에 쓸 레코드의 키의 값 직렬화하기 위해 사용하는 시리얼라이저 클래스
3. value.serializer : 카프카에 쓸 레코드의 밸류 값을 직렬화 하기 위해 사용하는 시리얼라이저 클래스

메시지 전송 방법

- 파이어 앤 포겟 : 메시지를 서버에 전송만 하고 성공 혹은 실패 여부에는 신경 쓰지 않는다
- 동기적 전송 : 메시지를 보내면 send()메서드는 Future 객체를 리턴하고 get() 메서드로 작업이 완료 될때까지 기다렸다가 성공 여부를 확인 한다
- 비동기적 전송 : 콜백 함수와 함께 send() 메서드를 호출하면 카프카 브로커로부터 응답을 받는 시점에서 자동으로 콜백 함수가 호출됨

### 3.4 프로듀서 설정하기

필수 항목 몇가지만

- [client.id](http://client.id/)
- acks : 0, 1, all
- 메시지 전달 시간 : !중요! 그림 참고해서 봐야함
    - [max.block.ms](http://max.block.ms/) -> [linqfer.ms](http://linqfer.ms/) -> -> [retry.backoff.ms](http://retry.backoff.ms/) -> [request.timeout.ms](http://request.timeout.ms/)
    - send() -> Batching -> Await send -> Retries -> Inflight
    - `delivery.timeout.ms >= linger.ms + retry.backoff.ms + request.timeout.ms`
    1. [max.block.ms](http://max.block.ms/) : 프로듀서가 얼마나 오랫도앙ㄴ 블록되는지 결정, 해당 시간만큼 흐르면 예외 발생
    2. [delibery.timeout.ms](http://delibery.timeout.ms/) : 레코드 전송 준비가 완료 된 시점에서부터 브로커의 응답을 받거나 아니면, 전송 포기하게 되는 시점까지의 제한시간을 결정함, request.timeout.ms보다 크지 않으면 예외 발생
    3. [request.timeout.ms](http://request.timeout.ms/) : 프로듀서가 데이터를 전송할 때 서버로부터 응답을 받기 위해 얼마나 기다릴건지 결정
    4. retries, [retry.backoff.ms](http://retry.backoff.ms/) : 프로듀서가 서버로부터 에러 메시지를 받았을 때 이것이 일시적인 에러일 수도 있음. 프로듀서가 메시지 전송을 포기하고 에러를 발생시킬 때까지 메시지를 재전송하는 횟수 결정
- [linger.ms](http://linger.ms/) : 현재 배치를 전송하기 전까지 대기하는 시간 결정
- buffer.memory : 프로듀서가 메시지를 전송하기 전에 메시지를 대기시키는 버퍼의 크기를 결정
- compression.type : 기본적으로 메시지는 압축되지 않은 상태로 전송
- batch.size : 같은 파티션에 다수의 레코드가 전송될 경우 프로듀서는 이것들을 배치 단위로 모아서 한번에 전송할 메모리양 결정
- max.in.flight.requests.per.connection : 프로듀서가 서버로부터 응답을 받지 못한 상태에서 전송할 수 있는 최대 메시지의 수를 결정
- max.request.size : 프로듀서가 전송하는 쓰기 요청의 크기를 결정, 최대 크기 제한과 한번에 보낼 요청 메시지의 최대 개수 제한
- receive.buffer.bytes, send.buffer.bytes : 데이터를 읽거나 쓸 때 소켓이 사용하는 TCP 송수신 버퍼의 크기를 결정
- enable.idempotence : 정확히 한번의 의미 구조를 지원 시작

### 3.5 시리얼라이저

커스텀 시리얼라이저도 가능하고 라이브러리를 사용하는것도 가능하며 범용 직렬화 라이브러리를 추천하며 에이브로를 추천함

### 3.5.3 카프카에서 에이브로 레코드 사용하기

에이브로는 레코드를 읽을 떄 스키마 전체를 필요로 하기 떄문에 어딘가 스키마를 저장해 두기는 해야하는 문제를 해결하기 위해 '스키마 레지스트리'라 불리는 아키텍처 패턴을 사용함

### 3.6 파티션

기본 파티셔너 사용 중에 키 값이 null인 레코드가 주어질 경우, 레코드는 현재 사용 가능한 토픽의 파티션 중 하나에 랜덤하게 저장됨
각 파티션별로 저장되는 메시지 개수의 균형을 맞추기 위해 라운드 로빈 알고리즘이 사용됨

### 3.7 헤더

헤더의 주된 용도 중 하나는 메시지의 전달 내역을 기록하는 것이고 데이터가 생성된 곳의 정보를 헤더에 저장해 두면, 메시지를 파싱할 필요 없이 헤더에 심어진 정보만으로 메시지를 라우팅하거나 출처를 추적할 수 있는 것이다.

### 3.8 인터셉터

ProducerIntercepter
모든 애플리케이션에 동일한 작동을 집어 넣는다거나 아니면 원래 코드를 사용할 수 없는 상황에 인터셉터를 사용한다
인터셉터 사례로는 모니터링, 정보 추적, 표준 헤더 삽입 등이 있다

ProducerRecord<K, V> onSend(ProducerRecord<K, V> record)
: 프로듀서가 레코드를 브로커로 보내기 전, 직렬화되기 직전에 호출된다

void onAcknowledgement(RecordMetadata metadata, Exception exception)
카프카 브로커가 보낸 응답을 클라이언트가 받았을 때 호출 된다. 브로커가 보낸 응답을 변경 할 수 없지만, 그 안에 담긴 정보는 읽을 수 있다

### 3.9 쿼터, 스로틀링

카프카 브로커에는 쓰기/읽기 속도를 제한할 수 있는 기능이 있다. 한도(쿼터)를 설정해주면 되는데 3가지가 있음

1. 쓰기 쿼터
2. 읽기 쿼터
3. 요청 쿼터

# 4장 카프카 컨슈머 : 카프카에서 데이터 읽기

카프카에서 데이터를 읽는 애플리케이션은 토픽을 구독하고 구독한 토픽들로부터 메시지를 받기 위해 KafkaConsumer를 사용한다.
다른 메시지 전달 시스템하고 카프카의 데이터 읽는 것은 조금 다르다 이걸 이해하는 챕터가 될 예정이다.

## 4.1.1 카프카와 컨슈머 그룹

1. 애플리케이션은 컨슈머를 생성
2. 해당 토픽 구독
3. 메시지를 받기 시작 한 뒤 받은 메시지를 받아 검사하고 결과를 쓴다

이후 검사할 수 있는 속도보다 더 빠른 속도로 토픽에 메시지를 쓰게 된다면 어떻게 될까?
-> 만약 데이터를 읽고 처리하는 컨슈머가 하나뿐이라면 애플리케이션은 새로 추가되는 메시지의 속도를 따라잡을 수 없기 때문에 메시지 처리가 계속해서 뒤로 밀리게 될 것이다. 따라서, 우리는 토픽으로부터 데이터를 읽어 오는 작업을 확장할 수 있었야 한다. 여러 개의 프로듀서가 동일한 토픽에 메시지를 쓰듯이, 여러 개의 컨슈머가 같은 토픽으로부터 데이터를 분할해서 읽어올 수 있게 해야 한다.

카프카 컨슈머는 컨슈머 그룹의 일부로 작동함
동일한 컨슈머 그룹에 속한 여러 개의 컨슈머들이 동일한 토픽을 구독할 경우, 각각 컨슈머는 해당 토픽에서 서로 다른 파티션의 메시지를 받는다.

카프카의 주 디자인 목표 중 하나는 카프카 토픽에 쓰여진 데이터를 전체 조직 안에서 여러 용도로 사용할 수 있도록 만드는 것이었다.
이러한 경우 우리는 각각의 애플리케이션 전체 메시지의 일부만 받는게 아니라 전부 다 받도록 해야 한다.
그리고 이렇게 하려면, 애플리케이션이 각자의 컨슈머 그룹을 갖도록 해야 한다.
다른 전통적인 메시지 전달 시스템과는 다르게 카프카는 성능 저하 없이 많은 수의 컨슈머와 컨슈머 그룹으로 확장이 가능하다.

컨슈머 그룹은 다른 컨슈머 그룹과는 상관 없이 토픽의 파티션 정보를 전체 받게 된다.

## 4.1.2 컨슈머 그룹과 파티션 리밸런스

- 컨슈머에 할당된 파티션을 다른 컨슈머에게 할당해주는 작업을 '리밸런스'라고 한다.
-> 컨슈머 그룹에 속한 컨슈머들은 자신들이 구독하는 토칙의 파티션들에 대한 소유권을 공유한다.
새로운 컨슈머를 컨슈머 그룹에 추가하면 이전에 다른 컨슈머가 읽고 있던 파티션으로부터 메시지를 읽기 시작한다.
컨슈머가 종료되거나 크래시가 났을 경우에도 마찬가지로 해당 컨슈머가 컨슈머 그룹에서 나가면 원래 이 컨슈머가 읽던 파티션들은 그룹에 잔류한 나머지 컨슈머 중 하나가 대신 받아서 읽기 시작하는것이다.
컨슈머에 파티션을 재할당하는 작업은 컨슈머 그룹이 읽고 있는 토픽이 변경되었을 떄도 발생한다.(운영자가 토픽에 새로운 파티션을 추가했을 경우)
리밸런스는 컨슈머 그룹에 높은 가용성과 규모 가변성을 제공하는 기능이기 떄문에 매우 중요하지만 문제 없이 작업이 수행되고 있는 와중에는 그리 달갑지 않은 기능이다

리밸런스 파티션 할당 전략 2가지

1. 조급한 리밸런스 : 실행되는 와중에 모든 컨슈머는 읽기 작업을 멈추고 자신에게 할당된 모든 파티션에 대한 소유권을 포기한 뒤, 컨슈머 그룹에 다시 참여하여 완전히 새로운 파티션 할당을 전달받는다.
2. 협력적 리밸런스 : 한 컨슈머에게 할당되어 있던 파티션만을 다른 컨슈머에 재할당한다.
컨슈머는 해당 컨슈머 그룹의 그룹 코디네이터 역할을 지정받은 카프카 브로커에 하트비트를 전송함으로써 멤버십과 할당된 파티션에 대한 소유권을 유지한다.

@ 버전 3.1 이후의 컨슈머 리밸런스 : 3.1부터는 협력적 리밸런스가 기본값

## 4.1.3 정적 그룹 멤버십

기본적으로 컨슈머가 갖는 컨슈머 그룹의 멤버로서의 자격(멤버십)은 일시적, 컨슈머가 컨슈머 그룹을 떠나는 순간 해당 컨슈머에 할당되어 있던 파티션들은 해제되고, 다시 참여하면 새롱누 멤버 ID가 발급되면서 리밸런스 프로토콜에 의해 새로운 파티션들이 할당된다.

정적 그룹 멤버십은 애플리케이션이 각 컨슈머에 할당된 파티션의 내용물을 사용해서 로컬 상태나 캐시를 유지해야 할 떄 편리하다.
캐시를 재생성하는 것이 시간이 오래 걸릴 때, 컨슈머가 재시작할 때마다 이 작업을 반복하고 싶지 않을 경우, 각 컨슈머에 할당된 파티션들이 해당 컨슈머가 재시작한다고 해서 다른 컨슈머로 재할당되지는 않는다.
일정 기간 동안, 어떤 컨슈머도 이렇게 컨슈머를 잃어버린 파티션들로부터 메시지를 읽어오지 않을 것이기 때문에 정지되었던 컨슈머가 다시 돌아오면 이 파티션에 저장된 최신 메시지에서 한참 뒤에 있는 밀린 메시지부터 처리하게 된다. 따라서 이 파티션들을 할당 받은 컨슈머가 재시작했을 떄 밀린 메시지들을 따라잡을 수 있는지 확인할 필요가 있다.

## 4.2 카프카 컨슈머 생성하기

1. KafkaConsumer 인스턴스 생성
2. bootstrap.servers, key.deserializer, value.deserializer 속성 설정

## 4.3 토픽 구독하기

1. subscribe() 메서드로 구독

@ 만약 카프카 클러스터에 파티션이 매우 많다면(3만개 이상이라든지) 구독할 토픽을 필터링하는 작업은 클라이언트쪽에서 이루어진다
정규식을 사용한 구독은 브로커, 클라이언트, 네트워크 전체에 걸쳐 상당한 오버헤드를 발생시킴

## 4.4 폴링 루프

while(true) {
ConsumerRecords<String, String> records = consumer.poll(timerout)
for(ConsumerRecord<String, String> record : records) {
int updateCount = 1;
if(custCountryMap.containsKet(record.value())) {
updatedCount = custCountryMap.get(record.value()) +1;
}
custCountryMap.put(record.value(), updatedCount);
JSONObject json = new JSONObject(custCOuntryMap);
System.out.println(json.toString());
}
}

## 4.4.1 스레드 안전성

하나의 스레드에 하나의 컨슈머가 원칙
하나의 애플리케이션에서 동일한 그룹에 속하는 여러 개의 컨슈머를 운용하고 싶다면 스레드를 여러개 띄어서 각각에 컨슈머에 하나씩 돌리는 수밖에 없다.

@ 카프카 예전 버전의 경우 전체 메서드 시그니처는 poll(long)이었고 현재는 poll(Duration)이다.

## 4.5 컨슈머 설정하기

주요 속성 살펴보기

1. fetch.min.bytes
2. [fetch.max.wait.ms](http://fetch.max.wait.ms/)
3. fetch.max.bytes
4. max.poll.records
5. max.partition.fetch.bytes
6. [session.timeout.ms](http://session.timeout.ms/), [heartbeat.interval.ms](http://heartbeat.interval.ms/)
@ 버전 2.8 기준으로 [session.timeout.ms](http://session.timeout.ms/) 설정의 기본값은 10초였고 3.0부터는 45초임
결과적으로 heartbeat.interval.ms는 session.timeout.ms의 1/3정도로 설정하라는 더이상 유효하지 않지만 그렇다고 이 값을 성급하게 바꾸지는 말자. 여전히 리밸런스가 진행중인 컨슈머를 탐지해 내는 데 쓰임
7. [max.poll.interval.ms](http://max.poll.interval.ms/)
8. [default.api.timeout.ms](http://default.api.timeout.ms/)
9. [request.timeout.ms](http://request.timeout.ms/)
10. auto.offset.reset
11. enable.auto.commit
12. partition.asignment.strategy
    - Range
    - RoundRobin
    - Sticky
    - Cooperative Sticky
13. [client.id](http://client.id/)
14. client.rack
@ 가까운 랙에서 읽어오기 : 카프카가 처음 개발되었던 시점에는 데이터 센터에 가까운 물리적 서버를 작동시키는게 보통이었으나 클라우드화 되면서 물리적 서버 랙이라기보다 클라우드의 가용 영역을 가리키는 경우가 많아졌음
15. [group.instance.id](http://group.instance.id/)
16. receive.buffer.bytes, send.buffer.bytes
17. offsets.retention.minutes

## 4.6 오프셋과 커밋

카프카의 고유 특성 중 하나는 JMS 큐들이 하는 것처럼 컨슈머로부터의 응답을 받는 방식이 아니라 컨슈머가 카프카를 사용해서 각 파티션에서의 위치를 추적할 수 있게 함

오프셋 커밋 : 파티션에서의 현재 위치를 업데이트하는 작업
전통적인 메시지 큐와는 다르게 카프카는 레코드를 개별적으로 커밋하지 않는 대신, 컨슈머는 파티션에서 성공적으로 처리해 낸 마지막 메시지를 커밋함으로써 그 앞의 모든 메시지들 역시 성공적으로 처리되었음을 암묵적으로 나타낸다

오프셋으로 인해 발생할 수 있는 경우 2가지

- 재처리된 메시지 : 커밋된 오프셋이 클라이언트가 처리한 마지막 메시지의 오프셋보다 작을 경우, 마지막으로 처리 된 오프셋과 오프셋 사이의 메시지들은 두번 처리 되게 된다.
- 오프셋 사이에서 누락된 메시지 : 커밋된 메시지가 클라이언트가 실제로 처리한 마지막 메시지의 오프셋보다 클 경우, 마지막으로 처리된 오프셋과 커밋된 오프셋 사이의 모든 메시지들은 컨슈머 그룹에서 누락되게 된다.

@ 어느 오프셋이 커밋되는가
오프셋을 커밋할 때 자동으로 하건 커밋할 오프셋을 지정하지 않고 하든 상관없이, poll()이 리턴한 마지막 오프셋 바로 다음 오프셋을 커밋하는 것이 기본적인 작동이다. 수동으로 특정 오프셋을 커밋하거나 특정한 오프셋 위치를 탐색할 때는 이점을 명심하자

## 4.6.1 자동 커밋

오프셋을 커밋하는 가장 쉬운 방법은 컨슈머가 대신하도록 하는 것
enable.auto.commit = true로 하면 5초에 한번, poll()을 통해 메시지 중 마지막 메시지의 오프셋을 커밋
[auto.commit.interval.ms](http://auto.commit.interval.ms/) 설정을 잡아 줌으로서 바꿀 수 있음

## 4.6.2 현재 오프셋 커밋하기

commitSync()는 poll()에 의해 리턴된 마지막 오프셋을 커밋한다는 점

## 4.6.3 비동기적 커밋

commitAsync()로 비동기 처리
수동적 커밋의 단점 중 하나는 브로커가 커밋 요청에 응답할 때까지 애플리케이션이 블록된다는 점으로 처리량을 제한하게 됨

## 4.6.4 동기적 커밋과 비동기적 커밋을 함꼐 사용하기

재시도 없이 커밋이 이따금 실패한다고 해서 큰 문제가 되지 않는다. 일시적인 문제일 경우 뒤이은 커밋이 성공할 것이기 떄문.
하지만 이것이 컨슈머를 닫기 전 혹은 리밸런스 전 마지막 커밋이라면, 성공여부를 추가로 '확인'할 필요가 있어서 동기적, 비동적을 함께 사용한다.

## 4.6 특정 오프셋 커밋하기

private Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>(); 을 이용해서 오프셋 관리를 한다

## 4.7 리밸런스 리스너

- public void onPartitionAssigned(Collection<TopicPartition> partitions)
파티션이 컨슈머에게 재할당된 후에, 하지만 컨슈머가 메시지를 읽기 시작히기 전에 호출, 파티션과 함께 사용할 상태를 적재하거나, 필요한 오프셋을 탐색하거나 등과 같은 준비 작업을 수행하는 곳이며 컨슈머가 그룹에 문제 ㅇ벗이 조인하려면, 여기서 수행되는 몯느 준비 작업은 [max.pill.timeout.ms](http://max.pill.timeout.ms/) 안에서 완료되어야 함
- public void onPartitionRevoked(Collection<TopicPartition> partitions)
컨슈머가 할당받았던 파티션이 할당 해제될 떄 호출. 일반적인 상황으로서 조급한 리밸런스 알고리즘이 사용되었을 경우, 이 메서드는 컨슈머가 메시지 읽기를 멈춘 뒤에, 그리고 리밸런스가 시작되기 전에 호출 됨
협력적 리밸런스 사용 시 리밸런스가 완료 될 때, 컨슈머에서 할당 해제되어야 할 파티션들에 대해서만 호출되며 오프셋을 커밋해주어야 이 파티션을 다음에 할당받는 컨슈머가 시작할 지점을 알아낼 수 있다.
- public void onPartitionLost(Collection<TopicPartition> partitions)
협력적 알고리즘 리밸런스 알고리즘이 사용되었을 시, 할당된 파티션이 리밸런스 알고리즘에 의해 해제되기 전에 다른 컨슈머에 먼저 할당된 예외적인 상황에서만 호출됨. 여기서는 파티션과 함께 사용되었던 상태나 자원들을 정리해주어야 함. 이러한 작업을 수행할 떄는 주의할 필요가 있다는 점을 명심하고 파티션을 새로 할당 받은 컨슈머가 이미 상태를 저장했을 수도 있기 때문에 충돌을 피해야 할 것이다. 만약 이 메시지를 구현하지 않았을 경우. onPartitionRevoked()가 대신 호출 된다는 점을 기억하자

@ 협럭적 리밸런스 알고리즘을 사용하고 있다면 다음 사항을 명심

1. onPartitionAssigned()는 리밸런싱이 ㅂ라생할 때마다 호출. 리밸런스가 발생했음을 컨슈머에게 알려주는 할이지만 컨슈머에게 새로 할당된 파티션이 없을 경우 빈 목록과 함꼐 호출
2. onPartitionRevoked()는 일반적인 리밸런스 상황에서 호출되지만, 파티셔닝 특정 컨슈머에서 해제될 때에만 호출, 메서드가 호출될 때 빈 목록이 주어지는 경우는 없다.
3. onPartitionLost()는 예외적인 리밸런스 상황에서 호출되며, 주어지는 파티션들은 이 메서드가 호출되는 시점에서 이미 달느 컨슈머들에게 할당되어 있는 상태다.

## 4.8 특정 프셋이 레코드 읽어오기

- 파티션이 맨 앞에서부터 모든 메세지를 읽어오거나 : seekToBeggining(Collection<TopicPartition> tp)
- 앞의 메시지는 전부 건너뛰고 파티션에 새로 들어온 메시지부터 읽기를 시작하고자 하거나 : seekToEnd(Collection<TopicPartition> tp)

## 4.9 폴링 루프 벗어나는 법

컨슈머를 종료하고자 할 때, 컨슈머가 poll()을 오랫동안 기다리고 있더라고 즉시 루프를 탈출하고 싶다면 다른 스레드에서 consumer.wakeup()을 호출해주어야 한다.
만약 메인 스레드에서 컨슈머 루프를 돌고 있다면 ShutdownHook을 사용할 수 있다.

## 4.10 디시리얼라이저

@ Serdes 사용하기
카프카에서 org.apache.kafka.common.serialization.Serdes를 제공함

## 4.10.1 커스텀 디시리얼라이저

카프카 프로듀서는 데이터를 쓰기 전 커스텀 객체를 바이트 배열로 변환하기 위해 시리얼라이저가 필요하다
마찬가지로 카프카 컨슈머는 카프카로부터 받은 바이트 배열을 자바 객체로 변환하기 위해 디시리얼라이저가 필요함
StringDeseializer를 기본값으로 사용함

```
public class CustomerDeserializer implements Deserializer<Custom> {

@Override
public void configure(Map configs, boolean isKey) {
	}
}

```

## 4.10.2 Avro 디시리얼라이저 사용하기

props.put("value.descrializer", "io.confluent.kafka.serializers.KafkaAvroDescrializer") 설정

@ List<T> (디)시리얼라이즈하기
List, Set, MAp 같은 복잡한 자료구조를 기존에는 제공 안해서 에이브로같은 외부 라이브러리를 사용해야 했는데 3.0부터는 지원함

## 4.11 독립 실행 컨슈머(Strandalone consumer) : 컨슈머 그룹 없이 컨슈머를 사용해야 하는 이유와 방법

컨슈머 그룹은 컨슈머들에게 파티션을 자동으로 할당해주고 해당 그룹에 컨슈머가 추가되거나 제거될 경우 자동으로 리밸런싱을 해준다.
보통은 이 방식을 원하지만 더 단순한 방법을 원할 경우.
하나의 컨슈머가 모든 파티션으로부터 모든 데이터를 읽어와야 하거나, 토픽의 특정 파티션으로부터 데이터를 읽어와야 할 떄 있다. 이러한 경우는 컨슈머 그룹이나 리밸런스 기능이 필요하지 않다.
그냥 컨슈머에게 특정한 토픽과 파티션을 할당해주고, 메시지를 읽어서 처리하고, 필요한 경우 오프셋을 커밋하면 된다.
만약 컨슈머가 어떤 파티션을 읽어와야하는지 정확히 알고 있을 겨웅 토픽을 구독할 필요 없이 그냥 파티션을 스스로 할당 받으면 되며 컨슈머는 토픽을 구독하거나 스스로 파티션을 할당할 수 있지만 두가지를 동시에 할 수는 없다.

LList<PartitionInfo> partitionInfos = comsumer.partitionFor("topic") : 파티션 지정
comsumer.assign(partitions) : 읽고자 하는 파티션 호출

# 5장 kafkaAdmin
