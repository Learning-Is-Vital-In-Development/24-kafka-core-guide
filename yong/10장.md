# 10장 클러스터간 데이터 미러링하기

지금까지는 단일 카프카 클러스터를 설치하고, 운영하고, 사용하는 방법에 대해서 설명함

하나 이상의 클러스터로 구성하는 아키턱처가 필요한 경우에 대한 설명

2개 이상의 클러스터는 서로 다른 부분으로 속해있기에 다른쪽을 복사할 이유가 없는 경우

→ 서로 다른 서비스 수준 협약(SLA)나 워크로드 떄문에 하나의 클러스터에서 여러 용도로 커버할 수 없는 경우가 있고, 서로 다른 보안 요구조건이있는경우가 있고 이러한 경우는 여러 개의 별도 클러스터를 운영하는 단일 클러스터를 여러 개 운영하는 것과 같음

운영자가 상호 의존하는 클러스터 사이에 데이터를 지속적으로 복사해줘야 하는 경우

→ 대부분의 데이터베이스에서는 데이터 베이스 서버 간에 지속적으로 데이터 복사하는 행위를 **복제** 라고 함. 
복제 : 이미 같은 클러스터에 속한 카프카 노드 간의 데이터의 교환
미러링 : 카프카 클러스터 간의 데이터 복제
미러 메이커 : 아파치 카프카에는 클러스터간 데이터 복제를 수행하기 위한 툴

## 10.1 클러스터간 미러링 활용 사례

### 지역 및 중앙 클러스터

하나의 기업이 지리적으로 분산된 하나 이상의 데이터센터를 가지고 있을수 있으며, 각각의 데이터 센터에 카프카 클러스터가 설치 되어 있는 경우

### 고가용성와 재해 복구

애플리케이션이 하나의 카프카 클러스터만을 사용하고 다른 곳에서의 데이터는 필요로 하지 않는다 하더라도, 전체 클러스터가 어떠한 이유에서든 사용 불가능하게 될 가능성은 우려스러움

### 규제

여러 나라에서 사업을 운영하는 회사의 경우 국가별로 다른 법적, 규제적 요구 조건을 따르기 위해 나라마다 있는 카프카 클러스터별로 서로 다른 설정과 정책을 시행해야 할 수 있다.

어떠한 데이터는 엄격한 접근 제한과 함께 서로 분리된 클러스터에 저장한 뒤 그 중 일부는 보다 개방적인 접근 설정이 되어 있는 클러스터로 복제하는 식이다.

### 클라우드 마이그레이션

요즘은 사내 데이터센터와 클라우드 공급자 모두를 사용해서 비즈니스를 운영한다.

여러 리전에서 실행되거나 이중화를 위해 여러 클라우드 공급자의 서비스를 사용하기도 함. 

클라우드간에 마이그레이션이 필요한 경우 카프카 커넥트를 사용해서 데이터베이스의 변경 내역을 로컬 카프카 클러스터로 내보낸 뒤 이를 다시 클라우드 환경에서 실행되는 카프카 클러스터로 미러링함으로써 새로운 애플리케이션이 이 데이터를 사용할 수 있도록 할 수 있다.

### 엣지 클러스터로부터의 데이터 집적

유통, 통신, 물류에서 헬스케어에 이르는 여러 산업에서는 연결성이 제한된 소용기기를 사용해서 데이터를 생성한다. 가용성이 높은 집적용 클러스터를 사용한다면 많은 수의 엣지 클러스터로부터 수집된 데이터를 분석하는 등의 용도로 사용될 수. ㅣㅆ다. 

## 10.2 다중 클러스터 아키텍처

### 10.2.1 데이터센터간 통신의 현실적 문제들

1. 높은 지연 : 두 카프카 클러스터 간의 거리나 네트워크 홉 개수가 증가함에 따라 통신 지연 역시 증가
2. 제한된 대역폭 : 광역 통신망(WAN)은 일반적인 단일 데이터센터 내부보다 훨씬 낮은 대역폭을 가지며 사용 가능한 대역폭도 시시각각 변하는 특성을 가지고 있음
3. 더 높은 비용 : 클러스터 간의 통신에는 더 많은 비용이 든다

원격 데이터센터에 데이터를 쓰는 것은 피하는게 좋음 

원격 클러스터 간 복제가 필요한 경우, 원격 브로커 간의 통신과 원격 프로듀서 브로커 통신을 배제하면 원격 브로커-컨슈머 통신이 남게 되며 가장 안전한 형식의 클러스터간 통신이다 

데이터센터간 통신을 위해 카프카를 조정하는 방법에 대해서 논의 

1. 하나의 데이터센터당 한 개 이상의 클러스터를 설치
2. 각각의 데이터센터 간에 각각의 이벤트(에러로 인한 재시도를 제외하면) 정확히 한번씩 복제
3. 가능하다면, 원격 데이터센터에 쓰는 것보다 원격 데이터센터에서 읽어오는 것이 더 낫다

### 10.2.2 허브-앤-스포크 아키텍처

→ 여러 개의 로컬 카프카 클러스터와 한 개의 중앙 카프카 클러스터가 있는 상황을 상정

→ 단순한 형태로는 리더와 팔로워, 두 개의 클러스터만 사용하는 방식

데이터가 여러 개의 데이터센터에서 생성되는 반면, 일부 컨슈머는 전체 데이터를 사용해야 할 경우 사용 된다.
각각의 데이터센터에서 실행되는 애플리케이션이 해당 데이터센터의 로컬 데이터만을 사용할 수 있게 해줄뿐 모든 데이터센터에서 생성된 전체 데이터세트를 사용할 수 있도록 해주지는 않는다

장점 : 항상 로컬 데이터센터에서 데이터가 생성되고 각각의 데이터센터에 저장된 이벤트가 중앙 데이터센터로 단 한 번만 미러링된다는 점

미러링은 한방향으로만 진행되고 각각이 컨슈머는 언제나 같은 클러스터에서 데이터를 읽게 되므로 아키테ㄱ처는 배포, 성정, 모니렁이 간편 

단점 : 다른 데이터센터에 있는 데이터를 사용할 수 없음 

구성 시 참고 : 중앙 데이터센터에 각 지역 데이터센터별로 미러링 프로세스를 최소 한개 이상 설정해야하며 미러링 프로세스는 원격 지역 클러스터의 데이터를 읽어서 중앙 클러스터에 씀 

만약 다수의 데이터센터에 같은 이름의 토픽이 존재한다면, 이 토픽들의 모든 데이터를 중앙 클러스터의 같은 이름 토픽 안에 쓰거나, 아니면 각각의 데이터센터에서 미러링 된 이벤트를 서로 다른 토픽에 쓰거나 할 수 있다

### 10.2.3 액티브-액티브 아키텍처

→ 2개 이상의 데이터 센터가 전체 데이터의 일부 혹은 전체를 공유하면서, 각 데이터센터가 모두 읽기와 쓰기를 수행할 수 있어야할 경우에 사용

장점

1. 인근 데이터센터에서 사용자들의 요청을 처리할 수 있음 
2. 허브-앤-스포크 아키텍처럼 사용할 수 있는 데이터가 제한됨으로써 발생하는 기능 제한이 없으며 성능상으로도 이점이 있음
3. 데이터 중복과 회복 탄력성임

단점 : 데이터를 여러 위치에서 비동기적으로 읽거나 변경할 경우 발생하는 충돌을 피하는것이 어려움 

동일한 데이터세트를 여러 위치에서 비동기적으로 읽고 썼을 때 발생하는 문제점을 해결할 방법이 있다면 이 아키텍처를 권장

구성 시 참고 : 순환 미러링을 막아야 함. 

### 10.2.4 액티브-스탠바이 아키텍처

액티브-스탠바이 아키텍처 : 카프카 클러스터에 장애가 발생했을 때, 애플리케이션이 다른 클러스터와 통신하도록 함으로써 전체 업무가 마비되는 것을 방지하기 위해 사용

다중 클러스터에 대한 유일한 요구 조건에 특정한 상황의 재해 대비 뿐임

하나의 데이터센터 안에 두 개의 클러스터를 가지고 있는 경우를 생각해 보자 

클러스터를 사용할지라도 첫 번째 클러스터의 모든 데이터를 가지고 있다가 첫 번째 클러스터가 완전히 사용할 수 없게 될 경우 대신 사용할 수 있는 두 번째 클러스터가 필요할 수 있다.

더 중요한 문제는, 아파치 카프카에서 DR 클러스터로 어떻게 장애 복구 하는가

1. 재해 복구 계획하기 : 낮은 RPO 지연 시간이 짧은 실시간 미러링을 필요로 하며 0으로 만들려면 동기적 방식의 미러링을 수행해야함
    1. 복구 시간 목표(RTO) : 모든 서비스가 장애가 발생한 뒤 다시 작동을 재개할 때까지의 최대 시간 의미
    2. 복구 지점 목표(RPO) : 장애의 결과로 인해 데이터 유실될 수 있는 최대 시간 의미
2. 계획에 없던 장애 복구에서의 데이터 유실과 불일치
    1. 카프카 미러링 솔루션들은 모두 비동기적으로 동작하므로 DR 클러스터는 주 클러스터의 가장 최신 메시지를 가지고 있지 못함 
3. 장애 복구 이후 애플리케이션의 시작 오프셋
→ 다른 클러스터로 장애 복구를 할 때 가장 어려운 것 중 하나는 다른 클러스터로 옮겨간 애플리케이션이 데이터를 읽어오기 시작해야 하는 위치를 결정하는 일
    1. 자동 오프셋 재설정 : 카프카 컨슈머는 사전에 커밋된 오프셋이 없을 경우 어떻게 동작해야 하는지를 결정하는 설정 값을 갖는다 → 데이터의 맨 처음부터 읽기 시작해서 상당한 수의 많은 양의 데이터를 처리할 것인지, 아니면 맨 끝에서 시작해서 알려지지 않은 개수의 이벤트를 건너뛸 것인지? 중복 처리할 수 있거나 약간의 유실이 별문제가 안됨다면 선택은 쉬울것임 → 맨 끝에서부터 읽기 시작하는 것은 단순한만큼 많이 사용되는 방법이다 
    2. 오프셋 토픽 복제 
        1. 컨슈머는 자신의 오프셋을 _consumer_offset의 특별한 토픽에 커밋함
        2. 이 토픽을 DR 클러스터로 미러링 해준다면 DR 클러스터에서 읽기 작업을 시작하는 컨슈머는 이전에 주 클러스터에 마지막 커밋한 오프셋부터 작업을 재개함
    3. 시간 기반 장애 복구 
        1. 메시지는 카프카로 전송된 시각을 타임스탬프 값을 가짐 
        2. 타임스탬프를 기준으로 오프셋을 검색할 수 있는 인덱스와 API를 포함 
    4. 오프셋 변환 
        1. 오프셋 토픽을 미러링할 때 가장 어려운 것은 주 클러스터와 DR 클러스터의 오프셋이 어긋날 수 있다 
        2. 오프셋이 어긋날 경우, DR 클러스터에 이벤트가 복제될 때마다 미러링 툴이 서로 매핑되는 오프셋 값을 외부 저장소로 전송하는 식
4. 장애 복구가 끝난 후. 
    1. 장애가 생겼던 주 클러스터를 이제 DR 클러스터로 역할을 변경해야 함. 
    2. 단순히 미러링 프로세스의 방향만 반대로 바꾸고 새 주 클러스터에서 예전 주 클러스터로 미러링을 시작하면 되면 생기는 문제점 
        1. 어디서부터 미러링을 시작해야 하는지 어떻게 아는가? 중복이 발생하거나 유실이 발생하거나 둘 다. 발생할 수 있다는 점을 명심
        2. 이전 주 클러스터는 DR 클러스터가 가지고 있지 않은 이벤트를 가지고 있을 가능성이 높음, 새로운 데이터를 반대 방향으로 미러링 하기 시작한다면, 여분의 이벤트들은 여전히 이전 주 클러스터 안에 남아있을 것이고 두 클러스터의 내용물은 서로 달라지게 됨
    3. 일관성과 순서 보장이 극도로 중요한 상황에서 가장 가장 간단한 해법은 일단 원래 주 클러스터에 저장된 데이터와 커밋된 오프셋을 완전히 삭제한 뒤 새로운 클러스터에서 완전히 새것이 된 새 DR 클러스터로 미러링을 시작하는 것이다 
5. 클러스터 디스커버리 관련
    1. 스탠바이 클러스터를 준비할 때 고려해야할 때 중요한 하나 : 장애가 발생한 상황에서 애플리케이션이 장애 복구용 클러스터와 통신을 시작하는 방법을 알 수 있게 하는 것. 

### 10.2.5 스트레치 클러스터

데이터 센터 전체에 문제가 발생했을 때 카프카 클러스터에 장애가 발생하는 것을 방지 하지 위함

→ 하나의 카프카 클러스터를 여러 개의 테이터 센터에 걸쳐 설치함

스트레치 클러스터는 다중 클러스터가 아니고 단지 하나의 클러스터일 뿐이므로 두 클러스터를 동기화시켜주는 미러링 프로세스가 필요 없음 

카프카의 복제 메커니즘이 평소대로 클러스터 안의 브로커들을 동기화된 상태로 유지시켜 주어 동기적인 복제를 포함 

메시지가 두 데이터센터에 위치한 카프카 브로커 각각에게 성공적으로 쓰여진 뒤에야 응답이 가도록 설정 함. 

각각의 파티션이 하나 이상의 데이터센터에 분산해서 레플리카를 저장하도록 설정하고 min.insunc.replicas 설정을 잡아주고 acks 설정을 all로 잡아줌으로써 각각의 쓰기 작업이 최소 두개의 데이터센터에 성공한 다음에야 응답이 가도록 설정하면 됨

## 10.3 아파키 카프카의 미러메이커

두 데이터센터 간에 데이터 미러링을 위해 미러메이커라 불리는 툴을 포함함

미러메이커에서는 각각의 태스크가 한 쌍의 컨슈머와 프로듀서로 이루어진다. 

여러 태스크를 하나의 서버에서 수행할 수도 있고 여러 개의 서버에서 나눠서 수행할 수도 있음 

원본 클러스터의 각 파티션에 저장된 이벤트를 대상 클러스터의 동일한 파티션으로 미러링함으로써 파티션의 의미 구조나 파티션 안에서의 이벤트 순서를 그대로 유지함 

### 10.3.1 미러메이커 설정

1. 복제 흐름
    1. 액티브-스탠바이 복제 흐름을 정의 
2. 미러링 토픽
    1. 각 복제 흐름에서 미러링될 토픽들을 지정하기 위해 정규식 사용가능
3. 컨슈머 오프셋 마이그레이션
    1. 주 클러스터에서 DR 클러스터로 장애 복구를 수행할 때 주 클러스터에서 마지막으로 체크포인트된 오프셋을 DR 클러스터에서 찾을 수 있도록 RemoteClusterUtils 유틸리티 클래스를 포함
4. 토픽 설정 및 ACL 마이그레이션
    1. 데이터 레코드를 미러링 하는 것 외에도 토픽 설정과 접근 제어 목록(ACL)을 미러링하도록 설정 가능 
5. 커넥트 태스크
    1. tasks.max 설정 옵션은 미러 메이커를 띄울 수 있는 커넥터 태스크의 최대 갯수를 지정
    2. 기본 값 1, 최소 2 이상으로 올려잡을것을 권장
6. 설정 접두어
    1. 미러메이커에 사용되는 모든 컴포넌트들(커넥터, 프로듀서, 컨슈머, 어드민 클라이언트)에 대해서 설정 가능
    2. {클러스터}.{커넥터 설정}
    3. {클러스터}.admin.{어드민 클라이언트 설정}
    4. {원본 클러스터}.consumer.{컨슈머 설정}
    5. {대상 클러스터}.producer.{프로듀서 설정}
    6. {원본 클러스터}→{대상 클러스터}.{복제 흐름 설정}

### 10.3.2 다중 클러스터 토폴로지

미러메이커는 원격 토픽 앞에 클러스터 별칭을 붙임으로써 동일한 이벤트가 순환 복제 되는 것을 방지

### 10.3.3 미러메이커 보안

프로덕션 환경 클러스터의 경우, 모든 데이터센터간 트래픽에 보안을 적용하는 것이 중요 

원본과 대상 클러스터 양쪽에 대해 보안이 적용된 브로커 리스너를 사용하도록 설정되어야 함. 

각 클러스터에 대해 클라이언트 쪽 보안 옵션들 역시 설정되어야 함. 

모든 데이터센터간 트래픽에는 SSL 암호화가 적용되어야 함. 

### 10.3.4 프로덕션 환경에 미러메이커 배포하기

전용 모드 실해 시 프로세시를 여러개 띄움으로써 확장설과 내고장성을 갖춘 미러메이커 클러스터 구성

docker로도 구성이 가능하며 카프카 커넥트를 기반한 만큼 카프카 커넥트에서 실행 가능함 

‘지역 읽기 - 원격 쓰기’를 해야 하는 경우엔 어떠한 경우가 있을까 

→ 데이터센터 간에 전송될 때는 암호화를 해야 하지만 데이터 센터 안에서는 암호화가 필요하지 않음

1. 카프카 커넥트 모니터링 
    1. 커넥터 상태를 모니터링 하기 위한 커넥트 지표, 처리량을 모니터링하기 위한 소스 커넥터 지표, 리밸런스 지연을 모니터링하기 위한 워커 지표와 같이 서로 다른 측면을 모니터링 하기위한 넓은 범위의 지표를 제공
2. 미러메이커 지표 모니터링
    1. 미러 처리량과 복제 지연을 모니터링할 수 있는 지표들을 추가로 제공
    2. record-age-ms : 복제 시점에 레코드가 얼마나 오래되었는지를 보여준다
    3. byte-rate : 복제 처리량을 보여준다
    4. checkpoint-latency-ms : 오프셋 마이그레이션의 지연을 보여준다
    5. 주기적으로 하트비트를 내보는ㅁ
3. 랙 모니터링
    1. 대상 클러스터가 원본 클러스터에서 얼마나 뒤쳐저 있는지를 알고 싶으면 랙을 보면 됨 
    2. 미러메이커가 원본 카프카 클러스터에 마지막으로 커밋된 오프셋을 추적함
    3. 파티션의 마지막 이벤트 오프셋, 미러메이커가 커밋한 마지막 오프셋, 그리고 둘 사이의 랙 확인가능
    4. 커밋 여부와 상관 없이 미러메이커가 읽어 온 최신 오프셋을 추적함
4. 프로듀서/컨슈머 지표 모니터링 
    1. 미러메이커가 사용하는 카프카 커넥트 프레임워크는 프로듀서와 컨슈머를 포함 
    2. 컨슈머 
        1. fetch-size-avg
        2. fetch-size-max
        3. fetch-rate
        4. fetch-throttle-time-avg
        5. fetch-throttle-time-max
    3. 프로듀서
        1. batch-size-avg
        2. batch-size-max
        3. request-in-flight
        4. record-retry-rate
    4. 둘다
        1. io-ratio
        2. io-wait-ratio
5. 카타리아 테스트
    1. 1분에 한번, 소스 클러스터의 특정한 토픽에 이벤트를 하나 보낸 뒤 대상 클러스터의 토픽에서 해당 메시지를 읽는 식으로 구현
    2. 이벤트가 복제될 때까지 걸리는 시간이 일정 수준을 넘어 갔을 때 경보를 받을 수 있음 
    3. 이런 상황이 발생한다면 미러메이커가 랙에 시달리고 있거나 아니면 아예 작동 중이지 않음

### 10.3.5 미러메이커 튜닝하기

미러메이커는 수평 확장 가능한 시스템

필요한 처리량과 허용할 수 있는 랙의 크기에 의해 결정 됨 

랙을 전혀 허용할 수 없다면 최고 수준의 처리량을 유지할 수 있는 수준으로 미러메이커의 크기를 키워줘야 함

1. linger.ms, batch.size
    1. 프로듀서가 계속해서 부분적으로 빈 배치들을 전송하고 있다면(batch-size-avg와 batch-size-max 지표값이 batch.size보다 낮은 경우) 약간의 지연을 추가함으로써 처리량을 증대할 수 있음
2. max.in.flight.requests.per.connection
    1. 일부 메시지에 대해 성공적으로 응답이 올 때까지 몇 번의 재시도가 필요한 상황에서 미러메이커가 메시지 순서를 보존하는 방법은 전송중인 요청의 수를 1로 제한하는 방법뿐
    2. 프로듀서가 요청을 전송할 때마다 대상 클러스터로부터 응답이 오기 전까지는 다음 메시지를 보낼 수 없음 
3. fetch.max.bytes
    1. fetch-size-avg, fetch-size-max 지표가 fetch.max.bytes 설정 값과 비슷하다면 컨슈머는 브로커로부터 최대한 데이터를 읽어오는 것
4. fetch.min.bytes, fetch.max.wait.ms
    1. 만약 fetch-rate 지표가 높게 유지된다면, 컨슈머가 브로커에게 너무 많은 요청을 보내고 있는데 정작 각 요청에서 실제로 받은 데이터는 너무 적다는걸 의미 
    2. fetch.min.bytes와 fetch.max.wait.ms 설정을 늘려 잡아 준다 

## 10.4 기타 클러스터간 미러링 솔루션

### 10.4.1 우버 uReplicator

### 10.4.2 링크드인 브루클린

### 10.4.3 컨플루언트의 데이터센터간 미러링 솔루션

1. 컨플루언트 리플리케이터
    1. 미러메이커와 비슷한 카프카 커넥트 프레임워크에 기반한 미러링 툴
    2. 레플리케이터는 집적토픽이라는 개념
2. 멀티 리전 클러스터 
    1. 데이터 센터 간의 거리가 서로 가까울 뿐 아니라 데이터센터 간에 동기적 복제가 가능할 정도로 안정적인 저지연 네트워크가 설치되어있어야 함
    2. 멀티 리전 클러스터도 데이터센터간 지연이 50밀리초 이하인 경우에만 적절하지만 이 경우엔 동기적인 복제와 비동기적 복제를 조합함으로써 프로듀서 성능에 미치는 영향을 제한하고 더 높은 네트워크 내고장을 제공함 
    3. 컨플루언트 서버는 ISR 목록에 추가되지 않은 채 비동기적으로 데이터를 복제하는 레플리카인 옵저버의 개념 역시 도입함(ISR에 없는 만큼 acks=all 설정을 사용하는 프로듀서의 작동에 영향을 주지 않지만, 컨슈머에게 레코드를 전달해줄 수 있다)
3. 클러스터 링킹 
    1. 클러스터간 복제 기능을 아예 컨플루언트 서버에 탑재
    2. 클러스터 내 브로커간 복제와 동일한 프로토토콜을 사용하는 클러스터 링킹 기능은 서로 다른 클러스터 간에 오프셋을 보존한 채로 복제를 수행
